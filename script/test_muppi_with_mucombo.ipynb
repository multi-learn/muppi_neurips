{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b231d319",
   "metadata": {},
   "source": [
    "## MuPPI Dataset Tutorial: Proteins Classification with MuCombo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8021414",
   "metadata": {},
   "source": [
    "### Step 1: Install packages, import functions, and download MuPPI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b041ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install scikit multimodallearn  and summit-multi-learn module\n",
    "!pip install scikit multimodallearn\n",
    "!pip install summit-multi-learn\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c8f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import h5py\n",
    "import numpy as np\n",
    "from multimodal.boosting.combo import MuComboClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106bb4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!wget -O MuPPI.hdf5 \"https://huggingface.co/datasets/kossikevin/muppi/resolve/main/MuPPI_2025_14_fullview_EMF.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c23f10a",
   "metadata": {},
   "source": [
    "### Step 2: Create a not full version\n",
    "Initially, some samples in the dataset have missing values in some views. The function below selects only samples without missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62a7959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_not_full_version_from_full(full_path, output_path):\n",
    "    with h5py.File(full_path, \"r\") as f:\n",
    "        labels = f[\"Labels\"][:]\n",
    "        nb_views = f[\"Metadata\"].attrs[\"nbView\"]\n",
    "        example_ids = f[\"Metadata\"][\"example_ids\"][:].astype(str)\n",
    "        views = [f[f\"View{i}\"][:] for i in range(nb_views)]\n",
    "\n",
    "    # Identify samples with -1 (missing value) in each view\n",
    "    n_samples = labels.shape[0]\n",
    "    valid_mask = np.ones(n_samples, dtype=bool)\n",
    "    for view in views:\n",
    "        valid_mask &= ~(view == -1).any(axis=1)\n",
    "\n",
    "    print(f\"Valids samples : {valid_mask.sum()} / {n_samples}\")\n",
    "\n",
    "    filtered_labels = labels[valid_mask]\n",
    "    filtered_ids = example_ids[valid_mask]\n",
    "    filtered_views = [view[valid_mask] for view in views]\n",
    "\n",
    "    # Save in a new file\n",
    "    with h5py.File(output_path, \"w\") as f_out:\n",
    "        f_out.create_dataset(\"Labels\", data=filtered_labels)\n",
    "\n",
    "        for i, view_data in enumerate(filtered_views):\n",
    "            dset = f_out.create_dataset(f\"View{i}\", data=view_data)\n",
    "            dset.attrs[\"name\"] = f\"View{i}\"  # Ou autre nom si tu veux les garder\n",
    "            dset.attrs[\"sparse\"] = False\n",
    "\n",
    "        meta_grp = f_out.create_group(\"Metadata\")\n",
    "        meta_grp.attrs[\"nbView\"] = len(filtered_views)\n",
    "        meta_grp.attrs[\"nbClass\"] = len(set(filtered_labels))\n",
    "        meta_grp.attrs[\"datasetLength\"] = len(filtered_labels)\n",
    "        meta_grp.create_dataset(\"example_ids\", data=np.array(filtered_ids, dtype=\"S10\"))\n",
    "\n",
    "    print(f\"Dataset 'not full' version saved in: {output_path}\")\n",
    "\n",
    "create_not_full_version_from_full(\n",
    "    full_path=\"MuPPI.hdf5\",\n",
    "    output_path=\"MuPPI_NoNA.hdf5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da31ba",
   "metadata": {},
   "source": [
    "### Step 3: Start classification\n",
    "We will now test the performance of MuCombo, an algorithm designed for multi-view datasets with class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571fcdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with h5py.File(\"MuPPI_NoNA.hdf5\", \"r\") as f:\n",
    "    view_names = [v for v in f.keys() if \"View\" in v]\n",
    "    X_views = [f[x][:] for x in view_names]\n",
    "    y = f[\"Labels\"][:]  # shape (n_samples,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21645d22",
   "metadata": {},
   "source": [
    "In this tutorial, we focus on a binary classification task: distinguishing **multi_clustered** (label 1) from **EMF** (label 2).\n",
    "The third class, **mono_clustered**, is also present in the dataset with label 0, but it will not be used in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ff5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose task \n",
    "mask = np.isin(y, [1, 2]) # multi vs EMF\n",
    "X = np.concatenate([view[mask] for view in X_views], axis=1)\n",
    "y = y[mask]\n",
    "\n",
    "# Printing some stats\n",
    "print(f\"X shape: {X.shape}\")\n",
    "classes, counts = np.unique(y, return_counts=True)\n",
    "proportions = counts / counts.sum()\n",
    "for c, p in zip(classes, proportions):\n",
    "    print(f\"Classe {c}: {p:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the view indices requires by MuCombo fit method\n",
    "views_ind = [0]\n",
    "for view in X_views:\n",
    "    views_ind.append(views_ind[-1] + view.shape[1])\n",
    "print(views_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642acdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "scores = []\n",
    "for _ in range(10):\n",
    "    # Split des donn√©es\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=random_state)\n",
    "\n",
    "    # Fitting\n",
    "    clf = MuComboClassifier(random_state=random_state, n_estimators=200)\n",
    "    clf.fit(X_train, y_train, views_ind=views_ind)\n",
    "\n",
    "    # Evaluation\n",
    "    y_pred = clf.predict(X_test)\n",
    "    scores.append(f1_score(y_test, y_pred, average=\"binary\", pos_label=2))\n",
    "\n",
    "# Calculation of the confidence interval (95%)\n",
    "# SOURCE : https://medium.com/@ranjitmaity95/confidence-intervals-in-machinelearning-b727d9dbdfcd\n",
    "n_iterations = 1000\n",
    "bootstrapped_scores = []\n",
    "for _ in range(n_iterations):\n",
    "    sample = resample(scores, replace=True, random_state=random_state)\n",
    "    bootstrapped_scores.append(np.mean(sample))\n",
    "delta = (np.percentile(bootstrapped_scores, 97.5) - np.percentile(bootstrapped_scores, 2.5)) / 2\n",
    "print(f\"F1 score (EMF): {np.mean(bootstrapped_scores)} +/- {delta}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
